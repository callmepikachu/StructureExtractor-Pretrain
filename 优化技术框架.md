好的，这是为您精心设计的 `StructureExtractor-Pretrain` 项目中，融合了 **ContinualGNN** 和 **StreamE** 思想的长文档图构建模型的详细伪代码。每一行都力求清晰、准确，以便代码生成 Agent 能够复现整个流程。

---

### **全局变量与常量**

```python
# --- 全局变量 ---
G: Graph = initialize_empty_graph()  # 主图数据库 (e.g., Neo4j)
M: MemoryBuffer = initialize_memory_buffer(size=m)  # 记忆库 (独立内存缓冲区)
theta: GNNModel = initialize_GNN_model()  # GNN模型参数
F: FisherMatrix = None  # Fisher信息矩阵 (用于正则化)
memory_capacity: int = m  # 记忆库大小 (e.g., 250, 500)
regularization_weight: float = lambda  # 正则化权重 (e.g., 100, 200)
influence_threshold: float = delta  # 受影响节点检测的阈值
detection_hop: int = L  # 影响传播的跳数 (e.g., 2)
```

---

### **主流程：处理文档流**

```python
# --- 主函数 ---
def process_document_stream(document_chunks: List[Text]):
    """
    主函数：按顺序处理文档的每一个块（如段落）
    """
    for k, chunk_k in enumerate(document_chunks):
        # 1. 从文本块中抽取实体和关系
        entities_k, relations_k = extract_entities_and_relations(chunk_k)
        
        # 2. 构建当前时间步的增量图 ΔG_k
        delta_G_k = build_incremental_graph(entities_k, relations_k)
        
        # 3. 检测受此次变化影响的节点集合 I(ΔG_k)
        influenced_nodes = detect_influenced_nodes(delta_G_k, G, theta, L=detection_hop, delta=influence_threshold)
        
        # 4. 执行双重视角知识巩固，并更新模型和图
        #    这是核心，结合了数据回放和模型正则化
        update_graph_and_model(delta_G_k, influenced_nodes, M, theta, F, lambda=regularization_weight)
        
        # 5. 将增量图 ΔG_k 的结构合并到主图 G 中
        merge_delta_graph_into_main_graph(G, delta_G_k)
        
        # 6. 增量更新向量索引 (e.g., FAISS, Weaviate)
        #    只更新那些表示发生变化的节点
        updated_node_ids = get_updated_node_ids(influenced_nodes)  # 从更新后的表示中获取ID
        update_vector_index(updated_node_ids, get_node_embeddings(updated_node_ids))
        
        # 7. 更新记忆库 M，为下一次迭代做准备
        #    使用分层-重要性采样策略
        update_memory_buffer(M, influenced_nodes, memory_capacity)
    
    # 文档流处理完毕，G 即为最终的完整知识图
    return G
```

---

### **环节1：实体识别与关系抽取**

```python
# --- 环节1: 实体识别与关系抽取 ---
def extract_entities_and_relations(text_chunk: Text) -> Tuple[List[Entity], List[Relation]]:
    """
    使用预训练模型从文本块中抽取实体和关系。
    输出: (实体列表, 关系列表)
    """
    # 使用LUKE或DyGIE++模型
    model = load_pretrained_model("LUKE" or "DyGIE++")
    
    # 执行端到端的抽取
    entities = model.extract_entities(text_chunk)
    relations = model.extract_relations(text_chunk)
    
    return entities, relations
```

---

### **环节2：构建增量图 ΔG_k**

```python
# --- 环节2: 构建增量图 ---
def build_incremental_graph(entities: List[Entity], relations: List[Relation]) -> Graph:
    """
    将抽取的实体和关系转换为增量图结构。
    输出: ΔG_k = (ΔV_k, ΔE_k, ΔX_k)
    """
    delta_V_k = set()
    delta_E_k = set()
    delta_X_k = {}
    
    for entity in entities:
        # 对每个新实体进行实体消歧和链接
        linked_entity = entity_linking(entity, G)
        if linked_entity.is_new:
            # 如果是新实体，则添加到增量节点集
            delta_V_k.add(linked_entity.id)
            # 初始化其属性 (e.g., 名称、首次出现的上下文向量)
            delta_X_k[linked_entity.id] = initialize_node_attributes(entity)
        else:
            # 如果是已知实体，则可能需要更新其属性
            # 记录属性变更 (简化处理，实际中可能更复杂)
            old_attrs = G.get_node_attributes(linked_entity.id)
            new_attrs = update_attributes(old_attrs, entity.context)
            if old_attrs != new_attrs:
                delta_X_k[linked_entity.id] = new_attrs
    
    for relation in relations:
        # 假设关系抽取已经返回了链接后的实体ID
        head_id = relation.head.linked_id
        tail_id = relation.tail.linked_id
        rel_type = relation.type
        
        # 添加新边
        delta_E_k.add((head_id, rel_type, tail_id))
    
    # 构建并返回增量图对象
    delta_G_k = Graph(nodes=delta_V_k, edges=delta_E_k, attributes=delta_X_k)
    return delta_G_k
```

---

### **环节3：实体链接 (作为构建增量图的子步骤)**

```python
# --- 环节3: 实体链接 (子函数) ---
def entity_linking(candidate_entity: Entity, G: Graph) -> LinkedEntity:
    """
    判断候选实体是否对应图中已有节点。
    输出: 一个包含链接结果的对象 (LinkedEntity)
    """
    # 1. 候选生成 (Candidate Generation)
    candidates = generate_candidates(candidate_entity.name, G, top_k=50)
    # candidates 通过 BM25 (Elasticsearch) 和 BLINK (FAISS) 获得
    
    # 2. 候选排序 (Candidate Ranking)
    ranked_candidates = rank_candidates(candidate_entity, candidates, G)
    # 使用 BLINK 的 Cross-encoder 进行精排
    
    # 3. 决策
    best_candidate = ranked_candidates[0]
    if best_candidate.score > CONFIDENCE_THRESHOLD_HIGH:
        # 高置信度 -> 确定链接
        return LinkedEntity(id=best_candidate.node_id, is_new=False)
    elif best_candidate.score > CONFIDENCE_THRESHOLD_LOW:
        # 中等置信度 -> 软链接，等待更多信息
        return create_soft_link(candidate_entity, best_candidate)
    else:
        # 低置信度 -> 新增实体
        new_id = generate_new_node_id()
        return LinkedEntity(id=new_id, is_new=True)
```

---

### **环节4：检测受影响节点**

```python
# --- 环节4: 检测受影响节点 (核心: ContinualGNN 近似算法) ---
def detect_influenced_nodes(delta_G_k: Graph, G: Graph, theta: GNNModel, L: int, delta: float) -> Set[NodeID]:
    """
    使用近似算法高效检测受增量图影响的节点。
    输出: 受影响节点集合 I(ΔG_k)
    """
    influenced_nodes = set()
    
    # 根据论文 Lemma 4.1, 我们需要计算每个变化源对其他节点的影响
    
    # 1. 处理节点属性变更 (ΔX_k)
    for node_id, new_attrs in delta_G_k.attributes.items():
        old_attrs = G.get_node_attributes(node_id)
        delta_x_i = new_attrs - old_attrs  # 属性变化向量
        
        # 计算 W_tilde (所有GNN层权重的乘积)
        W_tilde = compute_W_tilde(theta)
        
        # 从 node_id 开始进行 L-hop BFS，计算 f_t,l_u
        f_scores = perform_bfs_from_source(G, node_id, L)
        
        # 计算每个节点 u 的影响度 Δh_u
        for u in f_scores:
            delta_h_u_L = f_scores[u] * (delta_x_i @ W_tilde)
            influence_degree = norm(delta_h_u_L)
            if influence_degree > delta:
                influenced_nodes.add(u)
    
    # 2. 处理新增/删除的边 (ΔE_k) 和新增节点 (ΔV_k)
    # 论文的近似方法: 将结构变化视为在节点上的一次性属性变化
    structural_changes = delta_G_k.edges.union(delta_G_k.nodes)
    
    for change in structural_changes:
        # 计算该变化在GNN第一层产生的表示变化 Δh_i^1
        delta_h_i_1 = estimate_structural_change_impact(change, G, theta)
        
        # 将 Δh_i_1 视为一个虚拟的属性变化
        # 从变化源节点开始进行 L-hop BFS
        source_node_id = get_source_node_id(change)
        f_scores = perform_bfs_from_source(G, source_node_id, L)
        
        # 快速近似每个节点 u 的影响度
        for u in f_scores:
            approx_influence = f_scores[u] * norm(delta_h_i_1)
            if approx_influence > delta:
                influenced_nodes.add(u)
    
    return influenced_nodes
```

---

### **环节5：双重视角知识巩固与模型更新**

```python
# --- 环节5: 核心更新函数 ---
def update_graph_and_model(delta_G_k: Graph, influenced_nodes: Set[NodeID], 
                          memory: MemoryBuffer, theta: GNNModel, 
                          F: FisherMatrix, lambda: float):
    """
    结合数据回放和模型正则化，更新GNN模型和节点表示。
    """
    # 1. 数据视角: 从记忆库 M 中加载节点
    replay_nodes = memory.get_nodes()
    
    # 2. 模型视角: 计算或更新 Fisher 信息矩阵 F
    #    使用记忆库中的节点来近似计算
    F = approximate_fisher_matrix(replay_nodes, theta)
    
    # 3. 定义总损失函数 (公式 17)
    def total_loss_function():
        # a. 新知识损失 (在受影响节点上)
        loss_new = 0
        for node_id in influenced_nodes:
            pred = theta.forward(G, node_id)  # 使用当前图G和模型theta进行前向传播
            true_label = G.get_node_label(node_id)
            loss_new += cross_entropy_loss(pred, true_label)
        
        # b. 历史知识损失 (在记忆库节点上回放)
        loss_data = 0
        for node_id in replay_nodes:
            pred = theta.forward(G, node_id)
            true_label = G.get_node_label(node_id)
            loss_data += cross_entropy_loss(pred, true_label)
        
        # c. 模型正则化损失 (防止灾难性遗忘)
        loss_model = 0
        for i, param in enumerate(theta.parameters):
            loss_model += F[i] * (param - theta.previous_params[i])**2
        
        total_loss = loss_new + loss_data + lambda * loss_model
        return total_loss
    
    # 4. 使用SGD优化总损失，更新模型参数
    optimizer = SGD(theta.parameters, lr=0.01)
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        loss = total_loss_function()
        loss.backward()
        optimizer.step()
    
    # 5. 更新受影响节点的表示 (可选: StreamE 风格的增量更新)
    #    这里我们直接使用更新后的模型生成新表示
    for node_id in influenced_nodes:
        new_embedding = theta.get_node_embedding(G, node_id)
        G.update_node_embedding(node_id, new_embedding)
    
    # 6. 保存当前模型参数，为下一次的正则化做准备
    theta.previous_params = copy.deepcopy(theta.parameters)
```

---

### **环节6：更新记忆库**

```python
# --- 环节6: 更新记忆库 (分层-重要性采样) ---
def update_memory_buffer(memory: MemoryBuffer, influenced_nodes: Set[NodeID], capacity: int):
    """
    使用分层-重要性采样策略更新记忆库。
    """
    for node_id in influenced_nodes:
        # 计算该节点的重要性
        importance = calculate_node_importance(node_id, G)
        
        # 计算采样概率 (公式 11)
        # 假设我们有节点的类别 (e.g., 社区或标签)
        cluster_k = G.get_node_cluster(node_id)
        p_cluster = get_cluster_sampling_probability(cluster_k, memory, G)
        p_importance = importance
        
        # 综合概率
        sampling_probability = p_cluster * (1 + alpha * p_importance)
        
        # 使用蓄水池抽样 (Reservoir Sampling) 在线更新
        memory.add_with_reservoir_sampling(node_id, sampling_probability, capacity)

def calculate_node_importance(node_id: NodeID, G: Graph) -> float:
    """
    计算节点重要性: 与邻居标签不同的比例。
    """
    neighbors = G.get_neighbors(node_id)
    if not neighbors:
        return 0.0
    
    node_label = G.get_node_label(node_id)
    different_count = sum(1 for n in neighbors if G.get_node_label(n) != node_label)
    importance = different_count / len(neighbors)
    return importance
```

---

### **环节7：合并增量图到主图**

```python
# --- 环节7: 合并增量图 ---
def merge_delta_graph_into_main_graph(G: Graph, delta_G_k: Graph):
    """
    将增量图 ΔG_k 的结构 (节点和边) 合并到主图 G 中。
    注意: 节点表示的更新已在 update_graph_and_model 中完成。
    """
    # 添加新节点
    for node_id in delta_G_k.nodes:
        if not G.has_node(node_id):
            G.add_node(node_id, attributes=delta_G_k.attributes.get(node_id, {}))
    
    # 添加新边
    for (head_id, rel_type, tail_id) in delta_G_k.edges:
        G.add_edge(head_id, tail_id, type=rel_type)
    
    # 更新节点属性
    for node_id, attrs in delta_G_k.attributes.items():
        if G.has_node(node_id):
            G.update_node_attributes(node_id, attrs)
```

---

### **环节8：更新向量索引**

```python
# --- 环节8: 增量更新向量索引 ---
def update_vector_index(node_ids: List[NodeID], embeddings: List[Vector]):
    """
    将更新后的节点向量增量地插入或更新到向量数据库。
    """
    vector_db = connect_to_vector_db("FAISS" or "Weaviate" or "Milvus")
    
    for node_id, embedding in zip(node_ids, embeddings):
        if vector_db.has_vector(node_id):
            # 如果已存在，则更新
            vector_db.update(node_id, embedding)
        else:
            # 如果是新向量，则插入
            vector_db.insert(node_id, embedding)
```

---
这份伪代码完整地复现了您优化后的技术框架，将 `ContinualGNN` 的持续学习机制和 `StreamE` 的解耦思想深度融入了长文档图构建的每一个环节。